The Transformer, a new network architecture, is proposed, relying solely on attention mechanisms and dispensing with recurrence and convolutions. Experiments on machine translation tasks show the model to be superior in quality, more parallelizable, and requiring less training time. It achieves state-of-the-art BLEU scores on English-to-German and English-to-French translation tasks, and generalizes well to English constituency parsing with both large and limited training data.
